{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwlZkcWkuGMD"
      },
      "source": [
        "# Deep Learning: CNN: Cancer Detection\n",
        "**Thomas Bohn**   --   **2025-09-14**\n",
        "\n",
        "{{xxxxx}}  \n",
        "\n",
        "--  [Main Report](xxxx)  --  [Github Repo](xxxx)  --  [Presentation Slides](xxx)  --  [Presentation Video](xxx) --  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07lWDJH6vAuV"
      },
      "source": [
        "# 1.&nbsp;Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhIXdBPvCDs"
      },
      "source": [
        "**Problem Statement**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Why is it Important?**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Limitations of Existing Solutions**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Contribution**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**DataSet**\n",
        "\n",
        "{{xxxxx}}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9augt6L_Wnb"
      },
      "source": [
        "## Python Libraries\n",
        "\n",
        "The following python libraries are used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "pjoVn_82_Rya",
        "outputId": "0c19792c-ae58-4e05-89bb-ac5327391e69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.13)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "# File system manangement\n",
        "import time, datetime, psutil, os\n",
        "import shutil\n",
        "import zipfile\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# Install text storage and manipulation\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import textwrap\n",
        "\n",
        "#Install Image processing\n",
        "from PIL import Image\n",
        "\n",
        "##################################\n",
        "\n",
        "# Plotting and visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "# Train-test split and cross validation\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#################################\n",
        "\n",
        "# Mount the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#################################\n",
        "\n",
        "# Import Tensor Flow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to TPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPU (Tensor Processing Unit) Setup for Accelerated Training\n",
        "# This code attempts to connect to Google's TPU infrastructure for faster model training\n",
        "# TPUs are specialized hardware designed specifically for machine learning workloads\n",
        "\n",
        "try:\n",
        "    # Try to detect and connect to a TPU cluster\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())  # Print the TPU master address\n",
        "    \n",
        "    # Connect to the TPU cluster\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    \n",
        "    # Initialize the TPU system for use\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    \n",
        "    # Create a TPU distribution strategy for multi-core TPU usage\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    \n",
        "except:\n",
        "    # Fallback: If TPU is not available, use the default strategy (CPU/GPU)\n",
        "    # This ensures the code works in environments without TPU access\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "# Print the number of replicas (cores) available for parallel processing\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "\n",
        "# Set up automatic tuning for data pipeline performance optimization\n",
        "# AUTOTUNE allows TensorFlow to automatically determine the optimal number of parallel calls\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    \n",
        "# Print TensorFlow version for reference\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y39B3sJjvTD1"
      },
      "source": [
        "## Global Variables\n",
        "\n",
        "The following are global variables referenced in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vfjyMW6K_SS-"
      },
      "outputs": [],
      "source": [
        "# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\n",
        "start = time.time()\n",
        "\n",
        "# Class representing the OS process and having memory_info() method to compute process memory usage\n",
        "process = psutil.Process(os.getpid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uLiPgQzL-Oiu",
        "outputId": "1daa3a4c-d833-4341-a5f4-e588be3b7163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Level of Detail for functions is set to: 2\n"
          ]
        }
      ],
      "source": [
        "# Global Debug flag used to turn on and off more chatty blocks of code\n",
        "gDEBUG = False\n",
        "if gDEBUG: print('Debug is set to:', gDEBUG)\n",
        "# Global Level of Detail of table stats and details\n",
        "gLOD = 2\n",
        "print('Level of Detail for functions is set to:', gLOD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnEP1IbB-1dF"
      },
      "source": [
        "# 2.&nbsp;Data Source\n",
        "\n",
        "In this section, the code loads the dataset from Google Drive.\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E54ZVhqGH0o1"
      },
      "source": [
        "## Import the Data (Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset Path Configuration for Kaggle Environment\n",
        "# Get the Google Cloud Storage (GCS) path for Kaggle datasets\n",
        "# This allows access to the competition datasets stored in Kaggle's cloud storage\n",
        "GCS_PATH = KaggleDatasets().get_gcs_path()\n",
        "\n",
        "# Load Monet Painting Dataset\n",
        "# Search for all TFRecord files containing Monet paintings in the monet_tfrec directory\n",
        "# TFRecord is TensorFlow's efficient binary format for storing large datasets\n",
        "MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n",
        "print('Monet TFRecord Files:', len(MONET_FILENAMES))\n",
        "\n",
        "# Load Photo Dataset  \n",
        "# Search for all TFRecord files containing regular photos in the photo_tfrec directory\n",
        "# These photos will be transformed into Monet-style paintings by the GAN\n",
        "PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n",
        "print('Photo TFRecord Files:', len(PHOTO_FILENAMES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the Data (Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s47OY0wa-9VR",
        "outputId": "73c66d41-1e90-4c72-dd74-eecd25769ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the source of the zipped data files\n",
        "target_file = 'gan-getting-started.zip'\n",
        "source_path_root =  '/content/drive/MyDrive/[1.4] MsDS Class Files/-- DTSA 5511 Deep Learning/data'\n",
        "destination_path_root = '/content'\n",
        "\n",
        "# Copy the files to the runtime\n",
        "shutil.copy(source_path_root+'/'+target_file, destination_path_root+'/')\n",
        "\n",
        "# Display the files in the destination directory\n",
        "print(os.listdir(destination_path_root+'/'))\n",
        "\n",
        "# Unzip the files (this is slow)\n",
        "zip_file_path = destination_path_root+'/'+target_file\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all the contents into the specified folder\n",
        "        zip_ref.extractall(destination_path_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset Path Configuration for Google Colab Environment\n",
        "# Set up local file paths for the extracted dataset files\n",
        "# This replicates the Kaggle GCS_PATH functionality for local Colab environment\n",
        "\n",
        "# Define the base path where the dataset was extracted\n",
        "COLAB_DATA_PATH = '/content/gan-getting-started'\n",
        "\n",
        "# Load Monet Painting Dataset (Colab Version)\n",
        "# Search for all TFRecord files containing Monet paintings in the monet_tfrec directory\n",
        "# Using os.path.join for cross-platform compatibility\n",
        "MONET_FILENAMES = tf.io.gfile.glob(os.path.join(COLAB_DATA_PATH, 'monet_tfrec', '*.tfrec'))\n",
        "print('Monet TFRecord Files:', len(MONET_FILENAMES))\n",
        "\n",
        "# Load Photo Dataset (Colab Version)\n",
        "# Search for all TFRecord files containing regular photos in the photo_tfrec directory\n",
        "# These photos will be transformed into Monet-style paintings by the GAN\n",
        "PHOTO_FILENAMES = tf.io.gfile.glob(os.path.join(COLAB_DATA_PATH, 'photo_tfrec', '*.tfrec'))\n",
        "print('Photo TFRecord Files:', len(PHOTO_FILENAMES))\n",
        "\n",
        "# Verify the datasets are loaded correctly\n",
        "if len(MONET_FILENAMES) > 0:\n",
        "    print(f\"✅ Successfully loaded {len(MONET_FILENAMES)} Monet painting files\")\n",
        "    print(f\"First Monet file: {MONET_FILENAMES[0]}\")\n",
        "else:\n",
        "    print(\"❌ No Monet files found. Check the dataset extraction path.\")\n",
        "\n",
        "if len(PHOTO_FILENAMES) > 0:\n",
        "    print(f\"✅ Successfully loaded {len(PHOTO_FILENAMES)} photo files\")\n",
        "    print(f\"First photo file: {PHOTO_FILENAMES[0]}\")\n",
        "else:\n",
        "    print(\"❌ No photo files found. Check the dataset extraction path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Datasets\n",
        "\n",
        "All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord. Finally, we define the function to extract the image from the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image Processing Configuration\n",
        "# Define the target image size for the GAN model\n",
        "IMAGE_SIZE = [256, 256]\n",
        "\n",
        "def decode_image(image):\n",
        "    # Decode the JPEG image data into a tensor with 3 color channels (RGB)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # Normalize pixel values from [0, 255] to [-1, 1] range\n",
        "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
        "    # Reshape to the target image size with 3 color channels\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    # Define the expected structure of each TFRecord entry\n",
        "    # This matches the format used in the Kaggle competition dataset\n",
        "    tfrecord_format = {\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # Filename as string\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),       # Image data as bytes\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.string)       # Target label as string\n",
        "    }\n",
        "    # Parse the TFRecord example according to the defined format\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    # Extract and decode the image data\n",
        "    image = decode_image(example['image'])\n",
        "    return image\n",
        "\n",
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    # Create a TFRecord dataset from the provided filenames\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    # Apply the read_tfrecord function to each example in parallel\n",
        "    # AUTOTUNE allows TensorFlow to automatically determine optimal parallelism\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr1LwEFwAUTh"
      },
      "outputs": [],
      "source": [
        "# Load in datasets.\n",
        "monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\n",
        "photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BtBwi2SKW2N"
      },
      "source": [
        "# 3.&nbsp;Exploratory Data Analysis (EDA)\n",
        "\n",
        "The EDA phase focuses on understanding the dataset, including data distribution and label counts. Additional a visual display of the images for the model are previewed to better understand the problem in scope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bO3CvKfI_eC"
      },
      "source": [
        "## EDA Analysis: Image Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a sample image\n",
        "example_monet = next(iter(monet_ds))\n",
        "example_photo = next(iter(photo_ds))\n",
        "\n",
        "# Visualize a photo example and a Monet example.\n",
        "plt.subplot(121)\n",
        "plt.title('Photo')\n",
        "plt.imshow(example_photo[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Monet')\n",
        "plt.imshow(example_monet[0] * 0.5 + 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SnbCeUGydTo"
      },
      "source": [
        "## EDA Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpqY9vM_yefS"
      },
      "source": [
        "ADD HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_4RFPD2fkeh"
      },
      "source": [
        "# 4.&nbsp; Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Upsample and Downsample Functions\n",
        "\n",
        "Our CycleGAN generator uses a **U-Net architecture** with encoder-decoder blocks. The **downsampling function** reduces spatial dimensions (width and height) while increasing feature channels using `Conv2D` with `stride=2`, which moves the filter 2 pixels at a time to halve the image resolution (256×256 → 128×128 → ... → 1×1). The **upsampling function** does the opposite, using `Conv2DTranspose` with `stride=2` to double the dimensions and reconstruct higher-resolution images from lower-resolution feature maps (1×1 → 2×2 → ... → 256×256).\n",
        "\n",
        "We use **instance normalization** instead of batch normalization because it's more effective for GAN training and style transfer tasks. It normalizes each feature map independently across spatial dimensions, providing better training stability. Since instance normalization isn't part of the standard TensorFlow API, we implement it using TensorFlow Add-ons.\n",
        "\n",
        "The U-Net architecture includes **skip connections** that preserve fine-grained details during upsampling by connecting corresponding encoder and decoder layers. This enables the generator to produce high-quality, detailed images by combining both low-level and high-level features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output Configuration\n",
        "# Number of channels in the final generated image (RGB = 3 channels)\n",
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def downsample(filters, size, apply_instancenorm=True):\n",
        "    # Weight initialization for stable training\n",
        "    # Xavier-like initialization with small standard deviation\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    # Build the downsampling block as a sequential model\n",
        "    result = keras.Sequential()\n",
        "    \n",
        "    # CONVOLUTIONAL LAYER\n",
        "    # Conv2D with stride=2 reduces spatial dimensions by half\n",
        "    # use_bias=False because we use instance normalization\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    # INSTANCE NORMALIZATION (Optional)\n",
        "    # Instance normalization helps with training stability in GANs\n",
        "    # It normalizes each feature map independently (unlike batch norm)\n",
        "    # This is particularly effective for style transfer tasks\n",
        "    if apply_instancenorm:\n",
        "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    # ACTIVATION FUNCTION\n",
        "    # LeakyReLU allows small negative values, helping with gradient flow\n",
        "    # This is preferred over ReLU in GAN discriminators and some generator parts\n",
        "    result.add(layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    # Weight initialization for stable training\n",
        "    # Same initialization as downsample for consistency\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    # Build the upsampling block as a sequential model\n",
        "    result = keras.Sequential()\n",
        "    \n",
        "    # TRANSPOSE CONVOLUTIONAL LAYER\n",
        "    # Conv2DTranspose with stride=2 doubles the spatial dimensions\n",
        "    # This is the inverse operation of Conv2D with stride=2\n",
        "    # use_bias=False because we use instance normalization\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "\n",
        "    # INSTANCE NORMALIZATION\n",
        "    # Always applied in upsampling blocks for training stability\n",
        "    # Helps maintain consistent feature distributions during reconstruction\n",
        "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    # DROPOUT (Optional)\n",
        "    # Applied to early upsampling layers to prevent overfitting\n",
        "    # Helps the model generalize better during training\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "\n",
        "    # ACTIVATION FUNCTION\n",
        "    # ReLU for upsampling blocks (different from LeakyReLU in downsample)\n",
        "    # ReLU helps with gradient flow during the reconstruction process\n",
        "    result.add(layers.ReLU())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ctkj7jNx4B"
      },
      "source": [
        "## Build The Generators\n",
        "\n",
        "The generator uses an encoder-decoder architecture where it first downsamples the input image to extract abstract features, then upsamples these features back to the original resolution. **Skip connections** bridge the encoder and decoder by directly connecting corresponding layers at the same resolution level. These connections help preserve fine-grained spatial details that would otherwise be lost during downsampling, enabling the generator to produce high-quality images with both global structure and local details. The skip connections also improve gradient flow during training, helping the model learn more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Generator():\n",
        "    # Input layer: expects RGB images of size 256x256\n",
        "    inputs = layers.Input(shape=[256,256,3])\n",
        "\n",
        "    # ENCODER STACK (Downsampling Path)\n",
        "    # Progressively reduces spatial dimensions while increasing feature channels\n",
        "    # Each layer halves the image size and doubles the channels (except first layer)\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)  - No instance norm on first layer\n",
        "        downsample(128, 4), # (bs, 64, 64, 128)      - Extract mid-level features\n",
        "        downsample(256, 4), # (bs, 32, 32, 256)      - Extract high-level features\n",
        "        downsample(512, 4), # (bs, 16, 16, 512)      - Extract semantic features\n",
        "        downsample(512, 4), # (bs, 8, 8, 512)        - Continue feature extraction\n",
        "        downsample(512, 4), # (bs, 4, 4, 512)        - High-level semantic features\n",
        "        downsample(512, 4), # (bs, 2, 2, 512)        - Very high-level features\n",
        "        downsample(512, 4), # (bs, 1, 1, 512)        - Bottleneck: most abstract features\n",
        "    ]\n",
        "\n",
        "    # DECODER STACK (Upsampling Path)\n",
        "    # Progressively increases spatial dimensions while decreasing feature channels\n",
        "    # Dropout is applied to first 3 layers to prevent overfitting during training\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)   - Start reconstruction with dropout\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)   - Continue with dropout\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)   - Continue with dropout\n",
        "        upsample(512, 4), # (bs, 16, 16, 1024)       - No dropout for stability\n",
        "        upsample(256, 4), # (bs, 32, 32, 512)        - Reduce channels, increase resolution\n",
        "        upsample(128, 4), # (bs, 64, 64, 256)        - Continue upsampling\n",
        "        upsample(64, 4), # (bs, 128, 128, 128)       - Final upsampling layer\n",
        "    ]\n",
        "\n",
        "    # FINAL OUTPUT LAYER\n",
        "    # Converts features back to RGB image with tanh activation for [-1,1] range\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)  # Xavier-like initialization\n",
        "    last = layers.Conv2DTranspose(\n",
        "        OUTPUT_CHANNELS,  # 3 channels for RGB output\n",
        "        4,                # 4x4 kernel size\n",
        "        strides=2,        # Double the spatial dimensions\n",
        "        padding='same',   # Maintain spatial dimensions\n",
        "        kernel_initializer=initializer,  # Weight initialization\n",
        "        activation='tanh' # Output in [-1, 1] range (standard for GANs)\n",
        "    ) # Final output: (bs, 256, 256, 3)\n",
        "\n",
        "    # FORWARD PASS IMPLEMENTATION\n",
        "    x = inputs\n",
        "\n",
        "    # ENCODER: Downsampling with skip connection collection\n",
        "    # Store intermediate features for skip connections (U-Net architecture)\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)           # Apply downsampling layer\n",
        "        skips.append(x)       # Store feature map for skip connection\n",
        "\n",
        "    # Prepare skip connections for decoder (reverse order, exclude bottleneck)\n",
        "    skips = reversed(skips[:-1])  # Skip the final 1x1 bottleneck layer\n",
        "\n",
        "    # DECODER: Upsampling with skip connections\n",
        "    # Combine upsampled features with corresponding encoder features\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)                                    # Upsample features\n",
        "        x = layers.Concatenate()([x, skip])          # Add skip connection (concatenate)\n",
        "\n",
        "    # Generate final output image\n",
        "    x = last(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the Discriminator\n",
        "\n",
        "The discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "    # Weight initialization for stable training\n",
        "    # Same initialization as generator for consistency\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    # Input layer: expects RGB images of size 256x256\n",
        "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "\n",
        "    x = inp\n",
        "\n",
        "    # FEATURE EXTRACTION LAYERS (Downsampling)\n",
        "    # Progressively reduce spatial dimensions while increasing feature channels\n",
        "    # First layer: No instance normalization (common practice for discriminators)\n",
        "    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)  - Extract low-level features\n",
        "    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)     - Extract mid-level features  \n",
        "    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)     - Extract high-level features\n",
        "\n",
        "    # FINAL CONVOLUTIONAL BLOCK\n",
        "    # Add zero padding to maintain spatial dimensions for the final conv layers\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256) - Add 1 pixel padding on all sides\n",
        "    \n",
        "    # Convolutional layer with stride=1 (no spatial reduction)\n",
        "    # 512 filters to extract the most abstract features\n",
        "    conv = layers.Conv2D(512, 4, strides=1,\n",
        "                         kernel_initializer=initializer,\n",
        "                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "    # Instance normalization for training stability\n",
        "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
        "\n",
        "    # LeakyReLU activation (standard for discriminators)\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\n",
        "\n",
        "    # OUTPUT LAYER PREPARATION\n",
        "    # Add zero padding for the final output layer\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512) - Add 1 pixel padding\n",
        "\n",
        "    # FINAL OUTPUT LAYER\n",
        "    # Single channel output (1 filter) for real/fake classification\n",
        "    # Each pixel in the output represents a patch classification\n",
        "    # No activation function - raw logits for loss calculation\n",
        "    last = layers.Conv2D(1, 4, strides=1,\n",
        "                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize CycleGAN Models\n",
        "\n",
        "This section creates the four core models that make up the CycleGAN architecture: two generators for bidirectional image translation and two discriminators for domain-specific quality control. All models are initialized within the TPU/GPU strategy scope to ensure optimal performance across available hardware resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize CycleGAN Models within TPU/GPU Strategy Scope\n",
        "# The strategy.scope() ensures all models are distributed across available hardware (TPU cores or GPUs)\n",
        "with strategy.scope():\n",
        "    # GENERATOR MODELS (Image-to-Image Translation)\n",
        "    # Two generators create a bidirectional mapping between photo and Monet domains\n",
        "    \n",
        "    monet_generator = Generator()  # Photo → Monet: Transforms regular photos into Monet-style paintings\n",
        "    photo_generator = Generator()  # Monet → Photo: Transforms Monet paintings into realistic photos\n",
        "\n",
        "    # DISCRIMINATOR MODELS (Real vs Fake Classification)\n",
        "    # Two discriminators ensure each generator produces convincing results in their target domain\n",
        "    \n",
        "    monet_discriminator = Discriminator()  # Monet Domain: Distinguishes real Monet paintings from generated ones\n",
        "    photo_discriminator = Discriminator()  # Photo Domain: Distinguishes real photos from generated ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Generator\n",
        "\n",
        "This section demonstrates the generator's transformation capability before training. Since our generators are not trained yet, the generated Monet-esque photo will not show the expected artistic style at this point. This provides a baseline to compare against the trained model's output later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Monet-style painting from example photo\n",
        "to_monet = monet_generator(example_photo)\n",
        "\n",
        "# Create side-by-side comparison visualization\n",
        "# Subplot 1: Display the original input photo\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Photo\")\n",
        "plt.imshow(example_photo[0] * 0.5 + 0.5)\n",
        "\n",
        "# Subplot 2: Display the generated Monet-style painting\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Monet-esque Photo\")\n",
        "plt.imshow(to_monet[0] * 0.5 + 0.5)\n",
        "\n",
        "# Display the comparison plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT6-MV_PN1CC"
      },
      "source": [
        "## Test Split Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3xCg6WhgqXI"
      },
      "source": [
        "# 5.&nbsp;Data Cleansing & Text Normalization\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap4Hw4S8Jd3E"
      },
      "source": [
        "## Core Normalization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gZaDQqdJoe_"
      },
      "source": [
        "## Apply Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm-_DGFFmuSv"
      },
      "source": [
        "# 6.&nbsp;Feature Engineering with TF-IDF\n",
        "\n",
        "The TfidfVectorizer from scikit-learn is used to convert the text documents into numerical features. The vectorizer transforms the collection of documents into a matrix of token counts, which is then normalized using the Term Frequency-Inverse Document Frequency (TF-IDF) transformation. This matrix representation of the text data serves as input to the machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWadgSEmJy8W"
      },
      "source": [
        "## TF_IDF Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whSsaEBx0a_L"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrJc1ZCi0IVf"
      },
      "source": [
        "# 7.&nbsp; Baseline Models: Supervised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO9FkdrHJ5US"
      },
      "source": [
        "## Model Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eMVteAb01Vs"
      },
      "source": [
        "## Build, Train, and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0vP9xMCnepL"
      },
      "source": [
        "# 8.&nbsp; Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYPbH5L1bjZ"
      },
      "source": [
        "## Tuning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtxev-Ul1pX9"
      },
      "source": [
        "## Execute Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qsNOvyUoeyj"
      },
      "source": [
        "# 9.&nbsp;Final Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka8gqTlWKIl_"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II9qX_tf2LSL"
      },
      "source": [
        "## Train the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DueXwork2PiL"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CQ4nuAfqRJA"
      },
      "source": [
        "## Explore Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSKrlnc92lEz"
      },
      "source": [
        "# 10.&nbsp;Scale the Auto-Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZDLjLP53aIq"
      },
      "source": [
        "## Auto-Classifier Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad5blW7Os2iX"
      },
      "source": [
        "## Rerun Process for L1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhOtlzh0jGGe"
      },
      "source": [
        "## Rerun Process for L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWv340Xx265l"
      },
      "source": [
        "# 11.&nbsp; Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuHSlWe-O-br"
      },
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNzNW_nH37_9"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Result Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YlcWEw_8Bp-"
      },
      "source": [
        "\n",
        "**Baseline Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Hyperparameter Tuning Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Best Model Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Best Model Performance**\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQfc7PI38Tq"
      },
      "source": [
        "## Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql8QLBc2LoEy"
      },
      "source": [
        "### Model Comparisons and Findings\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Baseline Results\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Hyperparameter Tuning\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Best Model Results\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Performance Breakdown (Best Model)\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlXB6itm4D4r"
      },
      "source": [
        "## Concluding Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGbGMDoNOr-w"
      },
      "source": [
        "## Patterns and Conclusions Across the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ofvfgBOsLZ"
      },
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb2X9yUX4FOn"
      },
      "source": [
        "# 12.&nbsp; References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8tcmLa4FXR"
      },
      "source": [
        "**Kaggle Competition**\n",
        "\n",
        "- [1] Amy Jang, Ana Sofia Uzsoy, and Phil Culliton. I’m Something of a Painter Myself. https://kaggle.com/competitions/gan-getting-started, 2020. Kaggle.\n",
        "\n",
        "**Documentation and References**\n",
        "\n",
        "- [2] Kaggle. Monet CycleGAN Tutorial. Amy Jang. 2020. https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial\n",
        "- [3] Kaggle. Week 5 GAN Mini-Project - Final Execution. Ryan Lynch. 2025. https://www.kaggle.com/code/lynchrl/week-5-gan-mini-project-final-execution\n",
        "\n",
        "**Library Documentation**\n",
        "\n",
        "- [4] https://keras.io/examples/generative/cyclegan/\n",
        "- [5] https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "- [6] https://www.tensorflow.org/guide/autodiff\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMWZ5mItuBlVRpXzeiXWthd",
      "collapsed_sections": [
        "r9augt6L_Wnb",
        "6BtBwi2SKW2N"
      ],
      "mount_file_id": "1h_D7cL2gGhflQZVfZeEActICb25iEJfa",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
