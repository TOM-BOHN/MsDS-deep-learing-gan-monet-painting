{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwlZkcWkuGMD"
      },
      "source": [
        "# Deep Learning: CNN: Cancer Detection\n",
        "**Thomas Bohn**   --   **2025-09-14**\n",
        "\n",
        "{{xxxxx}}  \n",
        "\n",
        "--  [Main Report](xxxx)  --  [Github Repo](xxxx)  --  [Presentation Slides](xxx)  --  [Presentation Video](xxx) --  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07lWDJH6vAuV"
      },
      "source": [
        "# 1.&nbsp;Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhIXdBPvCDs"
      },
      "source": [
        "**Problem Statement**\n",
        "\n",
        "The challenge is to build a Generative Adversarial Network (GAN) that can transform regular photographs into Monet-style paintings. This involves creating a system that can learn and replicate the distinctive artistic style of Claude Monet, including his unique color choices, brush strokes, and overall aesthetic. The goal is to generate 7,000 to 10,000 Monet-style images that are convincing enough to potentially fool classifiers into believing they are authentic Monet paintings.\n",
        "\n",
        "**Why is it Important?**\n",
        "\n",
        "This project demonstrates the intersection of art and artificial intelligence, showing how machine learning can be used to understand and replicate artistic styles. Beyond the technical challenge, this work has implications for art education, digital art creation, and our understanding of how AI can learn aesthetic principles. It also serves as a practical application of GANs in the domain of style transfer and image generation, which has applications in entertainment, education, and creative industries.\n",
        "\n",
        "**Limitations of Existing Solutions**\n",
        "\n",
        "Traditional style transfer methods often produce results that lack the nuanced artistic qualities of the target style. Many existing approaches struggle with maintaining the distinctive characteristics that make an artist's work recognizable, such as Monet's impressionistic brushwork and color palette. Additionally, most solutions require paired training data or produce artifacts that make the generated images look artificial rather than artistically authentic.\n",
        "\n",
        "**Contribution**\n",
        "\n",
        "This project implements a CycleGAN architecture to create a bidirectional mapping between the domain of regular photographs and Monet-style paintings. The approach uses unpaired training data, making it more practical than methods requiring exact correspondences between input and output images. The model learns to preserve the essential content of photographs while transforming their style to match Monet's artistic characteristics, creating convincing artistic renditions.\n",
        "\n",
        "**DataSet**\n",
        "\n",
        "The dataset consists of two main collections: 300 Monet paintings (256x256 pixels) and 7,028 regular photographs (256x256 pixels), both available in JPEG and TFRecord formats. The Monet paintings serve as the target style reference, while the photographs provide the source content for transformation. The dataset is designed to train a GAN that can learn the stylistic differences between these two domains and generate Monet-style versions of the input photographs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9augt6L_Wnb"
      },
      "source": [
        "## Python Libraries\n",
        "\n",
        "The following python libraries are used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjoVn_82_Rya",
        "outputId": "d1544109-8c86-48db-ba62-76e1c2dc296b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# File system manangement\n",
        "import time, datetime, psutil, os\n",
        "import shutil\n",
        "import zipfile\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# Install text storage and manipulation\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import textwrap\n",
        "\n",
        "#Install Image processing\n",
        "from PIL import Image\n",
        "\n",
        "##################################\n",
        "\n",
        "# Plotting and visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "# Train-test split and cross validation\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#################################\n",
        "\n",
        "# Mount the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aJv78ONa-mBp"
      },
      "outputs": [],
      "source": [
        "#################################\n",
        "\n",
        "# Import Tensor Flow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# this package was discontinued\n",
        "# tf.keras.FUNCTION should replace tfa\n",
        "#import tensorflow_addons as tfa\n",
        "\n",
        "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "    from kaggle_datasets import KaggleDatasets\n",
        "\n",
        "#################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlPVwDDh-mBp"
      },
      "source": [
        "## Connect to TPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w2jdspCJ-mBp",
        "outputId": "ab618aec-6fb8-4a78-fc9f-7c2b9df2c327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of replicas: 1\n",
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "# TPU (Tensor Processing Unit) Setup for Accelerated Training\n",
        "# This code attempts to connect to Google's TPU infrastructure for faster model training\n",
        "# TPUs are specialized hardware designed specifically for machine learning workloads\n",
        "\n",
        "try:\n",
        "    # Try to detect and connect to a TPU cluster\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())  # Print the TPU master address\n",
        "\n",
        "    # Connect to the TPU cluster\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "\n",
        "    # Initialize the TPU system for use\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "    # Create a TPU distribution strategy for multi-core TPU usage\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "except:\n",
        "    # Fallback: If TPU is not available, use the default strategy (CPU/GPU)\n",
        "    # This ensures the code works in environments without TPU access\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "# Print the number of replicas (cores) available for parallel processing\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "\n",
        "# Set up automatic tuning for data pipeline performance optimization\n",
        "# AUTOTUNE allows TensorFlow to automatically determine the optimal number of parallel calls\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Print TensorFlow version for reference\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y39B3sJjvTD1"
      },
      "source": [
        "## Global Variables\n",
        "\n",
        "The following are global variables referenced in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vfjyMW6K_SS-"
      },
      "outputs": [],
      "source": [
        "# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\n",
        "start = time.time()\n",
        "\n",
        "# Class representing the OS process and having memory_info() method to compute process memory usage\n",
        "process = psutil.Process(os.getpid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLiPgQzL-Oiu",
        "outputId": "4e6969ee-32ec-46fd-f506-88e601a03537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug is set to: True\n",
            "Level of Detail for functions is set to: 2\n"
          ]
        }
      ],
      "source": [
        "# Global Debug flag used to turn on and off more chatty blocks of code\n",
        "gDEBUG = True\n",
        "if gDEBUG: print('Debug is set to:', gDEBUG)\n",
        "# Global Level of Detail of table stats and details\n",
        "gLOD = 2\n",
        "print('Level of Detail for functions is set to:', gLOD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnEP1IbB-1dF"
      },
      "source": [
        "# 2.&nbsp;Data Source\n",
        "\n",
        "This section handles dataset loading and preparation for the CycleGAN model. The code automatically detects the execution environment (Kaggle, Google Colab, or local) and loads the appropriate dataset files. For Kaggle, it accesses the competition datasets through Google Cloud Storage. For Colab, it mounts Google Drive and extracts the dataset files locally. The datasets consist of Monet paintings and regular photos, which will be used to train the bidirectional image translation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E54ZVhqGH0o1"
      },
      "source": [
        "## Import the Data (Kaggle or Colab)\n",
        "\n",
        "This section loads the dataset file paths for both photo and Monet painting collections. The datasets are kept separate to maintain clear domain boundaries for the CycleGAN training process. We'll load the TFRecord filenames first, then process the actual image data in subsequent steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LvIg26cq-mBr",
        "outputId": "9bd7e883-1663-4ae8-9338-4b5736d393bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "os.environ:  environ({'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.5.3.2-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'COLAB_JUPYTER_TRANSPORT': 'ipc', 'NV_NVML_DEV_VERSION': '12.5.82-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn9-cuda-12', 'CGROUP_MEMORY_EVENTS': '/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.22.3-1+cuda12.5', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.22.3-1', 'VM_GCE_METADATA_HOST': '169.254.169.253', 'MODEL_PROXY_HOST': 'https://mp.kaggle.net', 'HOSTNAME': '4a82d8e4eb4a', 'LANGUAGE': 'en_US', 'TBE_RUNTIME_ADDR': '172.28.0.1:8011', 'COLAB_TPU_1VM': '', 'GCE_METADATA_TIMEOUT': '3', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-12-5=12.5.3.2-1', 'NV_NVTX_VERSION': '12.5.82-1', 'COLAB_JUPYTER_IP': '172.28.0.12', 'NV_CUDA_CUDART_DEV_VERSION': '12.5.82-1', 'NV_LIBCUSPARSE_VERSION': '12.5.1.3-1', 'COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL': 'http://172.28.0.1:8013/', 'NV_LIBNPP_VERSION': '12.3.0.159-1', 'NCCL_VERSION': '2.22.3-1', 'KMP_LISTEN_PORT': '6000', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'ENV': '/root/.bashrc', 'PWD': '/', 'TBE_EPHEM_CREDS_ADDR': '172.28.0.1:8009', 'COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT': '30s', 'TBE_CREDS_ADDR': '172.28.0.1:8008', 'NV_CUDNN_PACKAGE': 'libcudnn9-cuda-12=9.2.1.18-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'COLAB_JUPYTER_TOKEN': '', 'LAST_FORCED_REBUILD': '20250623', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-12-5=12.5.82-1', 'NV_LIBNPP_PACKAGE': 'libnpp-12-5=12.3.0.159-1', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20', 'NV_LIBCUBLAS_DEV_VERSION': '12.5.3.2-1', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'COLAB_KERNEL_MANAGER_PROXY_HOST': '172.28.0.12', 'UV_BUILD_CONSTRAINT': '', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-12-5', 'NV_CUDA_CUDART_VERSION': '12.5.82-1', 'COLAB_WARMUP_DEFAULTS': '1', 'HOME': '/root', 'LANG': 'en_US.UTF-8', 'CUDA_VERSION': '12.5.1', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-5=12.5.3.2-1', 'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-12-5=12.5.1-1', 'UV_SYSTEM_PYTHON': 'true', 'COLAB_RELEASE_TAG': 'release-colab_20250916-060051_RC00', 'KMP_TARGET_PORT': '9000', 'KMP_EXTRA_ARGS': '--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/m-s-1rweodecvc2e6 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true ', 'UV_INSTALL_DIR': '/usr/local/bin', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-12-5=12.3.0.159-1', 'COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS': '/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-5', 'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000', 'CLOUDSDK_PYTHON': 'python3', 'NV_LIBNPP_DEV_VERSION': '12.3.0.159-1', 'NO_GCE_CHECK': 'False', 'PYTHONPATH': '/env/python', 'NV_LIBCUSPARSE_DEV_VERSION': '12.5.1.3-1', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_CUDNN_VERSION': '9.2.1.18-1', 'SHLVL': '0', 'NV_CUDA_LIB_VERSION': '12.5.1-1', 'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service', 'NVARCH': 'x86_64', 'UV_CONSTRAINT': '', 'PYTHONUTF8': '1', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn9-dev-cuda-12=9.2.1.18-1', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.22.3-1+cuda12.5', 'LD_LIBRARY_PATH': '/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'COLAB_GPU': '', 'NV_CUDA_NSIGHT_COMPUTE_VERSION': '12.5.1-1', 'GCS_READ_CACHE_BLOCK_SIZE_MB': '16', 'NV_NVPROF_VERSION': '12.5.82-1', 'LC_ALL': 'en_US.UTF-8', 'COLAB_FILE_HANDLER_ADDR': 'localhost:3453', 'PATH': '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer', 'NV_LIBNCCL_PACKAGE_VERSION': '2.22.3-1', 'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command', 'DEBIAN_FRONTEND': 'noninteractive', 'COLAB_BACKEND_VERSION': 'next', 'OLDPWD': '/', '_PYVIZ_COMMS_INSTALLED': '1', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'JPY_SESSION_NAME': 'https://github.com/TOM-BOHN/MsDS-deep-learing-gan-monet-painting/blob/main/deep-learing-gan-monet-painting.ipynb', 'JPY_PARENT_PID': '96', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'ENABLE_DIRECTORYPREFETCHER': '1', 'USE_AUTH_EPHEM': '1', 'COLAB_NOTEBOOK_ID': 'https://github.com/TOM-BOHN/MsDS-deep-learing-gan-monet-painting/blob/main/deep-learing-gan-monet-painting.ipynb', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'ENABLE_RUNTIME_UPTIME_TELEMETRY': '1', 'TF2_BEHAVIOR': '1', 'TPU_ML_PLATFORM': 'Tensorflow', 'TPU_ML_PLATFORM_VERSION': '2.19.0', 'TF_CPP_MIN_LOG_LEVEL': '1', 'QT_QPA_PLATFORM_PLUGIN_PATH': '/usr/local/lib/python3.12/dist-packages/cv2/qt/plugins', 'QT_QPA_FONTDIR': '/usr/local/lib/python3.12/dist-packages/cv2/qt/fonts'})\n",
            "Detected Google Colab environment - using local datasets\n"
          ]
        }
      ],
      "source": [
        "print('os.environ: ', os.environ)\n",
        "\n",
        "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "    print(\"Detected Kaggle environment - using Kaggle datasets\")\n",
        "elif 'COLAB_GPU' in os.environ or 'COLAB_TPU' in os.environ or 'COLAB_CPU' in os.environ:\n",
        "    print(\"Detected Google Colab environment - using local datasets\")\n",
        "else:\n",
        "    print(\"YIKES! I don't know where I am !!!!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5LqBl2j-mBr"
      },
      "outputs": [],
      "source": [
        "# Environment Detection and Dataset Loading\n",
        "# Detect whether we're running in Kaggle or Google Colab and load datasets accordingly\n",
        "\n",
        "# Check if we're in Kaggle environment\n",
        "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "    print(\"Detected Kaggle environment - using Kaggle datasets\")\n",
        "\n",
        "    # Dataset Path Configuration for Kaggle Environment\n",
        "    # Get the Google Cloud Storage (GCS) path for Kaggle datasets\n",
        "    # This allows access to the competition datasets stored in Kaggle's cloud storage\n",
        "    GCS_PATH = KaggleDatasets().get_gcs_path()\n",
        "\n",
        "    # Load Monet Painting Dataset\n",
        "    # Search for all TFRecord files containing Monet paintings in the monet_tfrec directory\n",
        "    # TFRecord is TensorFlow's efficient binary format for storing large datasets\n",
        "    MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n",
        "    print('Monet TFRecord Files:', len(MONET_FILENAMES))\n",
        "\n",
        "    # Load Photo Dataset\n",
        "    # Search for all TFRecord files containing regular photos in the photo_tfrec directory\n",
        "    # These photos will be transformed into Monet-style paintings by the GAN\n",
        "    PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n",
        "    print('Photo TFRecord Files:', len(PHOTO_FILENAMES))\n",
        "\n",
        "elif 'COLAB_GPU' in os.environ or 'COLAB_TPU' in os.environ or 'COLAB_CPU' in os.environ:\n",
        "    print(\"Detected Google Colab environment - using local datasets\")\n",
        "\n",
        "    # Define the source of the zipped data files\n",
        "    target_file = 'gan-getting-started.zip'\n",
        "    source_path_root = '/content/drive/MyDrive/[1.4] MsDS Class Files/-- DTSA 5511 Deep Learning/data'\n",
        "    destination_path_root = '/content'\n",
        "\n",
        "    # Copy the files to the runtime\n",
        "    shutil.copy(source_path_root + '/' + target_file, destination_path_root + '/')\n",
        "\n",
        "    # Display the files in the destination directory\n",
        "    print('Files in destination directory:', os.listdir(destination_path_root + '/'))\n",
        "\n",
        "    # Unzip the files (this is slow)\n",
        "    zip_file_path = destination_path_root + '/' + target_file\n",
        "\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all the contents into the specified folder\n",
        "        zip_ref.extractall(destination_path_root)\n",
        "\n",
        "    print('Dataset extraction completed')\n",
        "\n",
        "    # Dataset Path Configuration for Google Colab Environment\n",
        "    # Set up local file paths for the extracted dataset files\n",
        "    # This replicates the Kaggle GCS_PATH functionality for local Colab environment\n",
        "    COLAB_DATA_PATH = '/content/gan-getting-started'\n",
        "\n",
        "    # Load Monet Painting Dataset (Colab Version)\n",
        "    # Search for all TFRecord files containing Monet paintings in the monet_tfrec directory\n",
        "    # Using os.path.join for cross-platform compatibility\n",
        "    MONET_FILENAMES = tf.io.gfile.glob(os.path.join(COLAB_DATA_PATH, 'monet_tfrec', '*.tfrec'))\n",
        "    print('Monet TFRecord Files:', len(MONET_FILENAMES))\n",
        "\n",
        "    # Load Photo Dataset (Colab Version)\n",
        "    # Search for all TFRecord files containing regular photos in the photo_tfrec directory\n",
        "    # These photos will be transformed into Monet-style paintings by the GAN\n",
        "    PHOTO_FILENAMES = tf.io.gfile.glob(os.path.join(COLAB_DATA_PATH, 'photo_tfrec', '*.tfrec'))\n",
        "    print('Photo TFRecord Files:', len(PHOTO_FILENAMES))\n",
        "\n",
        "else:\n",
        "    print(\"Detected local environment - please ensure datasets are available\")\n",
        "    # Fallback for local development - you can customize this path\n",
        "    LOCAL_DATA_PATH = './data'  # Adjust this path as needed\n",
        "\n",
        "    MONET_FILENAMES = tf.io.gfile.glob(os.path.join(LOCAL_DATA_PATH, 'monet_tfrec', '*.tfrec'))\n",
        "    PHOTO_FILENAMES = tf.io.gfile.glob(os.path.join(LOCAL_DATA_PATH, 'photo_tfrec', '*.tfrec'))\n",
        "\n",
        "    print('Monet TFRecord Files:', len(MONET_FILENAMES))\n",
        "    print('Photo TFRecord Files:', len(PHOTO_FILENAMES))\n",
        "\n",
        "# Verify the datasets are loaded correctly\n",
        "if len(MONET_FILENAMES) > 0:\n",
        "    print(f\"Successfully loaded {len(MONET_FILENAMES)} Monet painting files\")\n",
        "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "        print(f\"First Monet file: {MONET_FILENAMES[0]}\")\n",
        "else:\n",
        "    print(\"No Monet files found. Check the dataset path.\")\n",
        "\n",
        "if len(PHOTO_FILENAMES) > 0:\n",
        "    print(f\"Successfully loaded {len(PHOTO_FILENAMES)} photo files\")\n",
        "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "        print(f\"First photo file: {PHOTO_FILENAMES[0]}\")\n",
        "else:\n",
        "    print(\"No photo files found. Check the dataset path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kKovy4Y-mBs"
      },
      "source": [
        "## Load the Datasets\n",
        "\n",
        "All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord. Finally, we define the function to extract the image from the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV_R4txv-mBs"
      },
      "outputs": [],
      "source": [
        "# Image Processing Configuration\n",
        "# Define the target image size for the GAN model\n",
        "IMAGE_SIZE = [256, 256]\n",
        "\n",
        "def decode_image(image):\n",
        "    # Decode the JPEG image data into a tensor with 3 color channels (RGB)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # Normalize pixel values from [0, 255] to [-1, 1] range\n",
        "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
        "    # Reshape to the target image size with 3 color channels\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    # Define the expected structure of each TFRecord entry\n",
        "    # This matches the format used in the Kaggle competition dataset\n",
        "    tfrecord_format = {\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # Filename as string\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),       # Image data as bytes\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.string)       # Target label as string\n",
        "    }\n",
        "    # Parse the TFRecord example according to the defined format\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    # Extract and decode the image data\n",
        "    image = decode_image(example['image'])\n",
        "    return image\n",
        "\n",
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    # Create a TFRecord dataset from the provided filenames\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    # Apply the read_tfrecord function to each example in parallel\n",
        "    # AUTOTUNE allows TensorFlow to automatically determine optimal parallelism\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr1LwEFwAUTh"
      },
      "outputs": [],
      "source": [
        "# Load in datasets.\n",
        "monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\n",
        "photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BtBwi2SKW2N"
      },
      "source": [
        "# 3.&nbsp;Exploratory Data Analysis (EDA)\n",
        "\n",
        "The EDA phase focuses on understanding the dataset, including data distribution and label counts. Additional a visual display of the images for the model are previewed to better understand the problem in scope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bO3CvKfI_eC"
      },
      "source": [
        "## EDA Analysis: Image Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jF9I9Jq-mBt"
      },
      "outputs": [],
      "source": [
        "# Get a sample image\n",
        "example_monet = next(iter(monet_ds))\n",
        "example_photo = next(iter(photo_ds))\n",
        "\n",
        "# Visualize a photo example and a Monet example.\n",
        "plt.subplot(121)\n",
        "plt.title('Photo')\n",
        "plt.imshow(example_photo[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Monet')\n",
        "plt.imshow(example_monet[0] * 0.5 + 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SnbCeUGydTo"
      },
      "source": [
        "## EDA Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpqY9vM_yefS"
      },
      "source": [
        "ADD HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_4RFPD2fkeh"
      },
      "source": [
        "# 4.&nbsp; Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9udxPzv-mBu"
      },
      "source": [
        "##  Upsample and Downsample Functions\n",
        "\n",
        "Our CycleGAN generator uses a **U-Net architecture** with encoder-decoder blocks. The **downsampling function** reduces spatial dimensions (width and height) while increasing feature channels using `Conv2D` with `stride=2`, which moves the filter 2 pixels at a time to halve the image resolution (256×256 → 128×128 → ... → 1×1). The **upsampling function** does the opposite, using `Conv2DTranspose` with `stride=2` to double the dimensions and reconstruct higher-resolution images from lower-resolution feature maps (1×1 → 2×2 → ... → 256×256).\n",
        "\n",
        "We use **instance normalization** instead of batch normalization because it's more effective for GAN training and style transfer tasks. It normalizes each feature map independently across spatial dimensions, providing better training stability. Since instance normalization isn't part of the standard TensorFlow API, we implement it using TensorFlow Add-ons.\n",
        "\n",
        "The U-Net architecture includes **skip connections** that preserve fine-grained details during upsampling by connecting corresponding encoder and decoder layers. This enables the generator to produce high-quality, detailed images by combining both low-level and high-level features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvaL24YS-mBu"
      },
      "outputs": [],
      "source": [
        "# Output Configuration\n",
        "# Number of channels in the final generated image (RGB = 3 channels)\n",
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def downsample(filters, size, apply_instancenorm=True):\n",
        "    # Weight initialization for stable training\n",
        "    # Xavier-like initialization with small standard deviation\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    # Build the downsampling block as a sequential model\n",
        "    result = keras.Sequential()\n",
        "\n",
        "    # CONVOLUTIONAL LAYER\n",
        "    # Conv2D with stride=2 reduces spatial dimensions by half\n",
        "    # use_bias=False because we use instance normalization\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    # INSTANCE NORMALIZATION (Optional)\n",
        "    # Instance normalization helps with training stability in GANs\n",
        "    # It normalizes each feature map independently (unlike batch norm)\n",
        "    # This is particularly effective for style transfer tasks\n",
        "    if apply_instancenorm:\n",
        "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    # ACTIVATION FUNCTION\n",
        "    # LeakyReLU allows small negative values, helping with gradient flow\n",
        "    # This is preferred over ReLU in GAN discriminators and some generator parts\n",
        "    result.add(layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    # Weight initialization for stable training\n",
        "    # Same initialization as downsample for consistency\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    # Build the upsampling block as a sequential model\n",
        "    result = keras.Sequential()\n",
        "\n",
        "    # TRANSPOSE CONVOLUTIONAL LAYER\n",
        "    # Conv2DTranspose with stride=2 doubles the spatial dimensions\n",
        "    # This is the inverse operation of Conv2D with stride=2\n",
        "    # use_bias=False because we use instance normalization\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "\n",
        "    # INSTANCE NORMALIZATION\n",
        "    # Always applied in upsampling blocks for training stability\n",
        "    # Helps maintain consistent feature distributions during reconstruction\n",
        "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    # DROPOUT (Optional)\n",
        "    # Applied to early upsampling layers to prevent overfitting\n",
        "    # Helps the model generalize better during training\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "\n",
        "    # ACTIVATION FUNCTION\n",
        "    # ReLU for upsampling blocks (different from LeakyReLU in downsample)\n",
        "    # ReLU helps with gradient flow during the reconstruction process\n",
        "    result.add(layers.ReLU())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ctkj7jNx4B"
      },
      "source": [
        "## Build The Generators\n",
        "\n",
        "The generator uses an encoder-decoder architecture where it first downsamples the input image to extract abstract features, then upsamples these features back to the original resolution. **Skip connections** bridge the encoder and decoder by directly connecting corresponding layers at the same resolution level. These connections help preserve fine-grained spatial details that would otherwise be lost during downsampling, enabling the generator to produce high-quality images with both global structure and local details. The skip connections also improve gradient flow during training, helping the model learn more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EARn_QPw-mBv"
      },
      "outputs": [],
      "source": [
        "def Generator():\n",
        "    # Input layer: expects RGB images of size 256x256\n",
        "    inputs = layers.Input(shape=[256,256,3])\n",
        "\n",
        "    # ENCODER STACK (Downsampling Path)\n",
        "    # Progressively reduces spatial dimensions while increasing feature channels\n",
        "    # Each layer halves the image size and doubles the channels (except first layer)\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)  - No instance norm on first layer\n",
        "        downsample(128, 4), # (bs, 64, 64, 128)      - Extract mid-level features\n",
        "        downsample(256, 4), # (bs, 32, 32, 256)      - Extract high-level features\n",
        "        downsample(512, 4), # (bs, 16, 16, 512)      - Extract semantic features\n",
        "        downsample(512, 4), # (bs, 8, 8, 512)        - Continue feature extraction\n",
        "        downsample(512, 4), # (bs, 4, 4, 512)        - High-level semantic features\n",
        "        downsample(512, 4), # (bs, 2, 2, 512)        - Very high-level features\n",
        "        downsample(512, 4), # (bs, 1, 1, 512)        - Bottleneck: most abstract features\n",
        "    ]\n",
        "\n",
        "    # DECODER STACK (Upsampling Path)\n",
        "    # Progressively increases spatial dimensions while decreasing feature channels\n",
        "    # Dropout is applied to first 3 layers to prevent overfitting during training\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)   - Start reconstruction with dropout\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)   - Continue with dropout\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)   - Continue with dropout\n",
        "        upsample(512, 4), # (bs, 16, 16, 1024)       - No dropout for stability\n",
        "        upsample(256, 4), # (bs, 32, 32, 512)        - Reduce channels, increase resolution\n",
        "        upsample(128, 4), # (bs, 64, 64, 256)        - Continue upsampling\n",
        "        upsample(64, 4), # (bs, 128, 128, 128)       - Final upsampling layer\n",
        "    ]\n",
        "\n",
        "    # FINAL OUTPUT LAYER\n",
        "    # Converts features back to RGB image with tanh activation for [-1,1] range\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)  # Xavier-like initialization\n",
        "    last = layers.Conv2DTranspose(\n",
        "        OUTPUT_CHANNELS,  # 3 channels for RGB output\n",
        "        4,                # 4x4 kernel size\n",
        "        strides=2,        # Double the spatial dimensions\n",
        "        padding='same',   # Maintain spatial dimensions\n",
        "        kernel_initializer=initializer,  # Weight initialization\n",
        "        activation='tanh' # Output in [-1, 1] range (standard for GANs)\n",
        "    ) # Final output: (bs, 256, 256, 3)\n",
        "\n",
        "    # FORWARD PASS IMPLEMENTATION\n",
        "    x = inputs\n",
        "\n",
        "    # ENCODER: Downsampling with skip connection collection\n",
        "    # Store intermediate features for skip connections (U-Net architecture)\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)           # Apply downsampling layer\n",
        "        skips.append(x)       # Store feature map for skip connection\n",
        "\n",
        "    # Prepare skip connections for decoder (reverse order, exclude bottleneck)\n",
        "    skips = reversed(skips[:-1])  # Skip the final 1x1 bottleneck layer\n",
        "\n",
        "    # DECODER: Upsampling with skip connections\n",
        "    # Combine upsampled features with corresponding encoder features\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)                                    # Upsample features\n",
        "        x = layers.Concatenate()([x, skip])          # Add skip connection (concatenate)\n",
        "\n",
        "    # Generate final output image\n",
        "    x = last(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wjo7h_P-mBv"
      },
      "source": [
        "## Build the Discriminator\n",
        "\n",
        "The discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atCGRz4s-mBv"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "    # Weight initialization for stable training\n",
        "    # Same initialization as generator for consistency\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    # Input layer: expects RGB images of size 256x256\n",
        "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "\n",
        "    x = inp\n",
        "\n",
        "    # FEATURE EXTRACTION LAYERS (Downsampling)\n",
        "    # Progressively reduce spatial dimensions while increasing feature channels\n",
        "    # First layer: No instance normalization (common practice for discriminators)\n",
        "    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)  - Extract low-level features\n",
        "    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)     - Extract mid-level features\n",
        "    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)     - Extract high-level features\n",
        "\n",
        "    # FINAL CONVOLUTIONAL BLOCK\n",
        "    # Add zero padding to maintain spatial dimensions for the final conv layers\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256) - Add 1 pixel padding on all sides\n",
        "\n",
        "    # Convolutional layer with stride=1 (no spatial reduction)\n",
        "    # 512 filters to extract the most abstract features\n",
        "    conv = layers.Conv2D(512, 4, strides=1,\n",
        "                         kernel_initializer=initializer,\n",
        "                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "    # Instance normalization for training stability\n",
        "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
        "\n",
        "    # LeakyReLU activation (standard for discriminators)\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\n",
        "\n",
        "    # OUTPUT LAYER PREPARATION\n",
        "    # Add zero padding for the final output layer\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512) - Add 1 pixel padding\n",
        "\n",
        "    # FINAL OUTPUT LAYER\n",
        "    # Single channel output (1 filter) for real/fake classification\n",
        "    # Each pixel in the output represents a patch classification\n",
        "    # No activation function - raw logits for loss calculation\n",
        "    last = layers.Conv2D(1, 4, strides=1,\n",
        "                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8saRi6Gh-mBv"
      },
      "source": [
        "## Initialize CycleGAN Models\n",
        "\n",
        "This section creates the four core models that make up the CycleGAN architecture: two generators for bidirectional image translation and two discriminators for domain-specific quality control. All models are initialized within the TPU/GPU strategy scope to ensure optimal performance across available hardware resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP3nX-G--mBw"
      },
      "outputs": [],
      "source": [
        "# Initialize CycleGAN Models within TPU/GPU Strategy Scope\n",
        "# The strategy.scope() ensures all models are distributed across available hardware (TPU cores or GPUs)\n",
        "with strategy.scope():\n",
        "    # GENERATOR MODELS (Image-to-Image Translation)\n",
        "    # Two generators create a bidirectional mapping between photo and Monet domains\n",
        "\n",
        "    monet_generator = Generator()  # Photo → Monet: Transforms regular photos into Monet-style paintings\n",
        "    photo_generator = Generator()  # Monet → Photo: Transforms Monet paintings into realistic photos\n",
        "\n",
        "    # DISCRIMINATOR MODELS (Real vs Fake Classification)\n",
        "    # Two discriminators ensure each generator produces convincing results in their target domain\n",
        "\n",
        "    monet_discriminator = Discriminator()  # Monet Domain: Distinguishes real Monet paintings from generated ones\n",
        "    photo_discriminator = Discriminator()  # Photo Domain: Distinguishes real photos from generated ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzuYy7MK-mBw"
      },
      "source": [
        "## Test the Generator\n",
        "\n",
        "This section demonstrates the generator's transformation capability before training. Since our generators are not trained yet, the generated Monet-esque photo will not show the expected artistic style at this point. This provides a baseline to compare against the trained model's output later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLmSdRVT-mBw"
      },
      "outputs": [],
      "source": [
        "# Generate Monet-style painting from example photo\n",
        "to_monet = monet_generator(example_photo)\n",
        "\n",
        "# Create side-by-side comparison visualization\n",
        "# Subplot 1: Display the original input photo\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Photo\")\n",
        "plt.imshow(example_photo[0] * 0.5 + 0.5)\n",
        "\n",
        "# Subplot 2: Display the generated Monet-style painting\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Monet-esque Photo\")\n",
        "plt.imshow(to_monet[0] * 0.5 + 0.5)\n",
        "\n",
        "# Display the comparison plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT6-MV_PN1CC"
      },
      "source": [
        "## Test Split Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCrFGVt8-mBw"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3xCg6WhgqXI"
      },
      "source": [
        "# 5.&nbsp;Data Cleansing & Text Normalization\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap4Hw4S8Jd3E"
      },
      "source": [
        "## Core Normalization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9puIiMO-mBx"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gZaDQqdJoe_"
      },
      "source": [
        "## Apply Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wenOwEF--mBy"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm-_DGFFmuSv"
      },
      "source": [
        "# 6.&nbsp;Feature Engineering with TF-IDF\n",
        "\n",
        "The TfidfVectorizer from scikit-learn is used to convert the text documents into numerical features. The vectorizer transforms the collection of documents into a matrix of token counts, which is then normalized using the Term Frequency-Inverse Document Frequency (TF-IDF) transformation. This matrix representation of the text data serves as input to the machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWadgSEmJy8W"
      },
      "source": [
        "## TF_IDF Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxrEKDf8-mBy"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whSsaEBx0a_L"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ggQNBF-mBy"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrJc1ZCi0IVf"
      },
      "source": [
        "# 7.&nbsp; Baseline Models: Supervised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO9FkdrHJ5US"
      },
      "source": [
        "## Model Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7RZCz3o-mBz"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eMVteAb01Vs"
      },
      "source": [
        "## Build, Train, and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BywEY_1A-mBz"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0vP9xMCnepL"
      },
      "source": [
        "# 8.&nbsp; Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYPbH5L1bjZ"
      },
      "source": [
        "## Tuning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Qvr4Y8-mB0"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtxev-Ul1pX9"
      },
      "source": [
        "## Execute Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS1Ld52_-mB0"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qsNOvyUoeyj"
      },
      "source": [
        "# 9.&nbsp;Final Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka8gqTlWKIl_"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PJH-qOQ-mB0"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II9qX_tf2LSL"
      },
      "source": [
        "## Train the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49q3yf8J-mB1"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DueXwork2PiL"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBktB44s-mB1"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CQ4nuAfqRJA"
      },
      "source": [
        "## Explore Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYUjcibq-mB1"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSKrlnc92lEz"
      },
      "source": [
        "# 10.&nbsp;Scale the Auto-Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZDLjLP53aIq"
      },
      "source": [
        "## Auto-Classifier Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lQ_hfn--mB2"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad5blW7Os2iX"
      },
      "source": [
        "## Rerun Process for L1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBYupxxc-mB2"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhOtlzh0jGGe"
      },
      "source": [
        "## Rerun Process for L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmF_8SGy-mB2"
      },
      "outputs": [],
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWv340Xx265l"
      },
      "source": [
        "# 11.&nbsp; Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuHSlWe-O-br"
      },
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNzNW_nH37_9"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex-wgWx8-mB3"
      },
      "source": [
        "### Model Result Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YlcWEw_8Bp-"
      },
      "source": [
        "\n",
        "**Baseline Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Hyperparameter Tuning Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Best Model Results**\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "**Best Model Performance**\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQfc7PI38Tq"
      },
      "source": [
        "## Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql8QLBc2LoEy"
      },
      "source": [
        "### Model Comparisons and Findings\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Baseline Results\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Hyperparameter Tuning\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Best Model Results\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Performance Breakdown (Best Model)\n",
        "\n",
        "{{xxxxx}}\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlXB6itm4D4r"
      },
      "source": [
        "## Concluding Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGbGMDoNOr-w"
      },
      "source": [
        "## Patterns and Conclusions Across the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ofvfgBOsLZ"
      },
      "source": [
        "{{xxxxx}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb2X9yUX4FOn"
      },
      "source": [
        "# 12.&nbsp; References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8tcmLa4FXR"
      },
      "source": [
        "**Kaggle Competition**\n",
        "\n",
        "- [1] Amy Jang, Ana Sofia Uzsoy, and Phil Culliton. I’m Something of a Painter Myself. https://kaggle.com/competitions/gan-getting-started, 2020. Kaggle.\n",
        "\n",
        "**Documentation and References**\n",
        "\n",
        "- [2] Kaggle. Monet CycleGAN Tutorial. Amy Jang. 2020. https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial\n",
        "- [3] Kaggle. Week 5 GAN Mini-Project - Final Execution. Ryan Lynch. 2025. https://www.kaggle.com/code/lynchrl/week-5-gan-mini-project-final-execution\n",
        "\n",
        "**Library Documentation**\n",
        "\n",
        "- [4] https://keras.io/examples/generative/cyclegan/\n",
        "- [5] https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "- [6] https://www.tensorflow.org/guide/autodiff\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vv9TKw8-mB5"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "r9augt6L_Wnb",
        "6BtBwi2SKW2N"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}